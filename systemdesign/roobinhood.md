# ðŸ“˜ Robinhood System Design (Senior-Level, 10â€“15 min Version)

## ðŸŽ¯ 1. Problem Scope

Design a simplified Robinhood-like trading platform that supports:

### **Functional Requirements**

* Live stock prices (low latency, scalable)
* Order management:

  * Create market/limit orders
  * Cancel orders
  * List all orders for a user

### **Non-Functional Requirements**

* Strong consistency for orders
* Low-latency price propagation (< 200 ms)
* High scale: 20M DAUs, thousands of symbols
* Minimize direct exchange connections

---

# ðŸ§± 2. Core Entities

```
User
Symbol
Order (market | limit)
Trade (from exchange feed)
```

---

# ðŸ“¡ 3. APIs

### **Price Subscription**

```
POST /subscribe
Body: { symbols: ["META", "AAPL"] }
Response: SSE stream
```

### **Create Order**

```
POST /order
Body: {
  type: "market" | "limit",
  symbol: "META",
  priceInCents: 52210,
  numShares: 10
}
```

### **Cancel Order**

```
DELETE /order/:id
```

### **List Orders**

```
GET /orders
```

Notes:

* Use integers (`priceInCents`) to avoid float precision issues.
* Auth via headers/JWT.

---

# ðŸ—ï¸ 4. High-Level Architecture

```mermaid
flowchart LR
    Client -- SSE --> SymbolService
    SymbolService -- subscribe --> Redis[(Redis Pub/Sub)]
    ExchangeFeed --> PriceProcessor -- publish --> Redis

    Client --> OrderService --> ExchangeAPI
    ExchangeAPI --> TradeProcessor --> OrderDB
```

---

# ðŸŸ¦ 5. Live Price Streaming (MAIN PATTERN)

### âŒ Option 1: Client â†’ Exchange polling

Too many connections, wasteful, slow.

### âŒ Option 2: Client â†’ Internal polling

Better but still inefficient + latency tied to polling interval.

### âœ… **Great Solution: SSE + Redis Pub/Sub**

### **Flow**

1. Client calls `/subscribe` with list of symbols.
2. SymbolService:

   * stores `symbol â†’ set(users)`
   * subscribes to Redis channels as needed
3. ExchangeFeed publishes price updates â†’ PriceProcessor â†’ Redis
4. SymbolService fans out to clients via SSE

### **Why SSE?**

* Perfect for one-way streaming
* Built-in auto-reconnect
* Lightweight vs WebSockets
* No need for duplex communication

---

# ðŸŸ© 6. Order Placement Workflow

```mermaid
sequenceDiagram
    Client->>OrderService: POST /order
    OrderService->>OrderDB: insert(status=pending)
    OrderService->>ExchangeAPI: submit order
    ExchangeAPI-->>OrderService: externalOrderId
    OrderService->>KVStore: externalOrderId â†’ (userId, orderId)
    OrderService->>OrderDB: update(status=submitted)
    OrderService-->>Client: success
```

### Why write `pending` before calling the exchange?

To ensure user-visible consistency even if we crash.

---

# ðŸŸ« 7. Order Updates (Trade Fills)

### Architecture

```mermaid
flowchart LR
   ExchangeFeed --> TradeProcessor --> KVStore --> OrderDB
```

### Flow

1. Exchange emits trade (fill/partial fill/cancel).
2. TradeProcessor reads event.
3. Looks up `(userId, orderId)` in KV store.
4. Updates the correct shard in OrderDB.

### Why KV Store?

Because OrderDB is partitioned by `userId`,
and we cannot query `externalOrderId` globally across shards.

Good options:

* RocksDB (embedded, durable, fast)
* DynamoDB
* Redis (only if AOF or snapshotting enabled)

---

# ðŸŸª 8. Consistency & Failure Handling

### **Create Order Failures**

| Failure                                | Handling                                                 |
| -------------------------------------- | -------------------------------------------------------- |
| DB insert fails                        | Return error                                             |
| Exchange submit fails                  | Mark order failed                                        |
| DB update fails after exchange success | Cleanup job reconciles with exchange using clientOrderId |

### **Cancel Order Failures**

| Failure                        | Handling                           |
| ------------------------------ | ---------------------------------- |
| DB cannot set `pending_cancel` | Return error                       |
| Exchange cancel fails          | Mark failed + cleanup job          |
| DB update fails                | Cleanup job validates via exchange |

### Cleanup Job Responsibilities

* Scan `pending` and `pending_cancel`
* Query exchange using `clientOrderId` (metadata)
* Fix state in OrderDB

---

# ðŸŸ§ 9. Key Tradeoffs Summary

### **Real-time Prices**

| Design          | Verdict                    |
| --------------- | -------------------------- |
| Poll Exchange   | âŒ Too many calls           |
| Poll Internal   | âŒ Latency + waste          |
| **SSE + Redis** | âœ… Best for scale + latency |

### **Order Dispatch**

| Option                         | Verdict                       |
| ------------------------------ | ----------------------------- |
| Direct client â†’ exchange       | âŒ Too many connections        |
| Queue-based dispatch           | âŒ Latency spikes during peak  |
| **OrderService + NAT gateway** | âœ… Low latency, simple, secure |

---

# ðŸŸ¦ 10. Final Architecture Overview

```mermaid
flowchart TB
 subgraph Price Updates
    ExchangeFeed --> PriceProcessor --> Redis --> SymbolService --> Client
 end

 subgraph Order System
    Client --> OrderService --> ExchangeAPI
    ExchangeAPI --> TradeProcessor --> KVStore --> OrderDB
 end
```

---

# ðŸŽ¤ Interview Tips (Senior-Level)

* Talk **latency constraints** early (sub-200ms updates).
* Emphasize **SSE > WebSockets** specifically because the client only receives data.
* Hit the **consistency story** stronglyâ€”pending â†’ submitted â†’ cleanup job.
* Explain why **KV store solves cross-shard lookup**.
* Mention **horizontal scaling** for SymbolService + OrderService.

---

If you want, I can also generate:
âœ… **Staff-level version**
âœ… **Mid-level simplified version**
âœ… **HTML dashboard version**
